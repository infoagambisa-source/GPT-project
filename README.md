# Build Your Own GPT (Mini GPT)

## Compute
- Training environment: Google Colab (CPU)
- CUDA available: (fill after check)

## Dataset
- Source: Project Gutenberg (public domain)
- Books used: 
    1. Moby Dick; Or, The Whale by Herman Melville 
    2. Alice's Adventures in Wonderland by Lewis Carroll 

## Model / Training hyperparameters
- batch_size:
- block_size:
- n_layer:
- n_head:
- n_embd:
- max_iters:
- learning_rate:

## Results
- Final train loss:
- Final val loss:
- Sample outputs saved in /samples
