{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebfc9186",
   "metadata": {},
   "source": [
    "# Build your own GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206b398",
   "metadata": {},
   "source": [
    "### 1. Asses Compute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0738e89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.10.0+cpu\n",
      "CUDA available: False\n",
      "CPU: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel\n",
      "Platform: Windows-10-10.0.19045-SP0\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"CPU:\", platform.processor())\n",
    "print(\"Platform:\", platform.platform())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce902c67",
   "metadata": {},
   "source": [
    "### 2. Choose and Download Dataset (Project Gutenberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "643a0500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset characters: 1389023\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MOBY-DICK;\n",
      "\n",
      "or, THE WHALE.\n",
      "\n",
      "By Herman Melville\n",
      "\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "ETYMOLOGY.\n",
      "\n",
      "EXTRACTS (Supplied by a Sub-Sub-Librarian).\n",
      "\n",
      "CHAPTER 1. Loomings.\n",
      "\n",
      "CHAPTER 2. The Carpet-Bag.\n",
      "\n",
      "CHAPTER 3. The Spouter-Inn.\n",
      "\n",
      "CHAPTER 4. The Counterpane.\n",
      "\n",
      "CHAPTER 5. Breakfast.\n",
      "\n",
      "CHAPTER 6. The Street.\n",
      "\n",
      "CHAPTER 7. The Chapel.\n",
      "\n",
      "CHAPTER 8. The Pulpit.\n",
      "\n",
      "CHAPTER 9. The Sermon.\n",
      "\n",
      "CHAPTER 10. A Bosom Friend.\n",
      "\n",
      "CHAPTER 11. Ni\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "\n",
    "BOOK_URLS = [\n",
    "    \"https://www.gutenberg.org/cache/epub/2701/pg2701.txt\",  # Moby Dick; Or, The Whale\n",
    "    \"https://www.gutenberg.org/cache/epub/11/pg11.txt\",    # Alice's Adventures in Wonderland\n",
    "]\n",
    "\n",
    "def download_text(url: str) -> str:\n",
    "    raw = request.urlopen(url).read()\n",
    "    text = raw.decode(\"utf-8\", errors=\"replace\")\n",
    "    start = text.find(\"*** START OF\")\n",
    "    end = text.find(\"*** END OF\")\n",
    "    if start != -1 and end != -1:\n",
    "        text = text[start:end]\n",
    "\n",
    "    return text\n",
    "\n",
    "text = \"\\n\\n\".join(download_text(u) for u in BOOK_URLS)\n",
    "\n",
    "print(\"Dataset characters:\", len(text))\n",
    "print(text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d0eac",
   "metadata": {},
   "source": [
    "### 3. Preprocess: vocabulary + encode/decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d317539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 103\n",
      "[74, 62, 59, 2, 70, 72, 63, 68, 57, 59, 73, 73, 2, 73, 67, 63, 66, 59, 58]\n",
      "the princess smiled\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# Create mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encode/decode functions\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "# Quick sanity test\n",
    "test_str = \"the princess smiled\"\n",
    "print(encode(test_str)[:40])\n",
    "print(decode(encode(test_str)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63430a3c",
   "metadata": {},
   "source": [
    "### 4. Tokenize entire dataset + train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc5db5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1389023\n",
      "Train tokens: 1250120\n",
      "Val tokens: 138903\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert full text to integer tokens\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(\"Total tokens:\", data.numel())\n",
    "\n",
    "# Split train/val\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(\"Train tokens:\", train_data.numel())\n",
    "print(\"Val tokens:\", val_data.numel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b14fe",
   "metadata": {},
   "source": [
    "### 5. Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e9e5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x batch shape: torch.Size([16, 32])\n",
      "y batch shape: torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define initial hyperparameters \n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "\n",
    "def get_batch(split: str):\n",
    "    data_split = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# Sanity check\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"x batch shape:\", xb.shape)\n",
    "print(\"y batch shape:\", yb.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
