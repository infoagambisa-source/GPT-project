{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebfc9186",
   "metadata": {},
   "source": [
    "# Build your own GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206b398",
   "metadata": {},
   "source": [
    "### 1. Asses Compute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0738e89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.10.0+cpu\n",
      "CUDA available: False\n",
      "CPU: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel\n",
      "Platform: Windows-10-10.0.19045-SP0\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"CPU:\", platform.processor())\n",
    "print(\"Platform:\", platform.platform())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce902c67",
   "metadata": {},
   "source": [
    "### 2. Choose and Download Dataset (Project Gutenberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "643a0500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset characters: 1389023\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MOBY-DICK;\n",
      "\n",
      "or, THE WHALE.\n",
      "\n",
      "By Herman Melville\n",
      "\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "ETYMOLOGY.\n",
      "\n",
      "EXTRACTS (Supplied by a Sub-Sub-Librarian).\n",
      "\n",
      "CHAPTER 1. Loomings.\n",
      "\n",
      "CHAPTER 2. The Carpet-Bag.\n",
      "\n",
      "CHAPTER 3. The Spouter-Inn.\n",
      "\n",
      "CHAPTER 4. The Counterpane.\n",
      "\n",
      "CHAPTER 5. Breakfast.\n",
      "\n",
      "CHAPTER 6. The Street.\n",
      "\n",
      "CHAPTER 7. The Chapel.\n",
      "\n",
      "CHAPTER 8. The Pulpit.\n",
      "\n",
      "CHAPTER 9. The Sermon.\n",
      "\n",
      "CHAPTER 10. A Bosom Friend.\n",
      "\n",
      "CHAPTER 11. Ni\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "\n",
    "BOOK_URLS = [\n",
    "    \"https://www.gutenberg.org/cache/epub/2701/pg2701.txt\",  # Moby Dick; Or, The Whale\n",
    "    \"https://www.gutenberg.org/cache/epub/11/pg11.txt\",    # Alice's Adventures in Wonderland\n",
    "]\n",
    "\n",
    "def download_text(url: str) -> str:\n",
    "    raw = request.urlopen(url).read()\n",
    "    text = raw.decode(\"utf-8\", errors=\"replace\")\n",
    "    start = text.find(\"*** START OF\")\n",
    "    end = text.find(\"*** END OF\")\n",
    "    if start != -1 and end != -1:\n",
    "        text = text[start:end]\n",
    "\n",
    "    return text\n",
    "\n",
    "text = \"\\n\\n\".join(download_text(u) for u in BOOK_URLS)\n",
    "\n",
    "print(\"Dataset characters:\", len(text))\n",
    "print(text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d0eac",
   "metadata": {},
   "source": [
    "### 3. Preprocess: vocabulary + encode/decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d317539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 103\n",
      "[74, 62, 59, 2, 70, 72, 63, 68, 57, 59, 73, 73, 2, 73, 67, 63, 66, 59, 58]\n",
      "the princess smiled\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# Create mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encode/decode functions\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "# Quick sanity test\n",
    "test_str = \"the princess smiled\"\n",
    "print(encode(test_str)[:40])\n",
    "print(decode(encode(test_str)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63430a3c",
   "metadata": {},
   "source": [
    "### 4. Tokenize entire dataset + train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc5db5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1389023\n",
      "Train tokens: 1250120\n",
      "Val tokens: 138903\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert full text to integer tokens\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(\"Total tokens:\", data.numel())\n",
    "\n",
    "# Split train/val\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(\"Train tokens:\", train_data.numel())\n",
    "print(\"Val tokens:\", val_data.numel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b14fe",
   "metadata": {},
   "source": [
    "### 5. Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e9e5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x batch shape: torch.Size([16, 32])\n",
      "y batch shape: torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define initial hyperparameters \n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "\n",
    "def get_batch(split: str):\n",
    "    data_split = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# Sanity check\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"x batch shape:\", xb.shape)\n",
    "print(\"y batch shape:\", yb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0c2ff5",
   "metadata": {},
   "source": [
    "### 6. Hyperparameters (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0732efbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# CPU-friendly hyperparameters\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 100\n",
    "\n",
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c2d9f",
   "metadata": {},
   "source": [
    "### 7. Loss estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ce73e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f425ea9",
   "metadata": {},
   "source": [
    "### 8. Transformer blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fc8165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size: int):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)      # (B, T, head_size)\n",
    "        q = self.query(x)    # (B, T, head_size)\n",
    "\n",
    "        # attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)  # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)    # (B, T, head_size)\n",
    "        out = wei @ v        # (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # (B, T, C)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple MLP.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca06459a",
   "metadata": {},
   "source": [
    "### 9. GPT Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5f81644",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx: (B, T)\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "\n",
    "        x = self.blocks(x)     # (B, T, C)\n",
    "        x = self.ln_f(x)       # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]  # crop context\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]        # last time step: (B, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)             # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cc04c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822375 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel().to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, \"M parameters\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
